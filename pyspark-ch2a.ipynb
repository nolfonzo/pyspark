{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Ingestion**\n",
    "* To install jdbc driver, download mariadb jar from mariadb.com\n",
    "* Copy mariadb jar into $SPARK_HOME/jars/\n",
    "* In Remote-WSL, change ~/.vscode-server\n",
    "  1. PATH=/home/nolfonzo/miniconda3/bin:$PATH                                                                                     \n",
    "  2. export SPARK_HOME=\"/home/nolfonzo/spark-3.2.1-bin-hadoop3.2\"\n",
    "  3. export PATH=\"$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH\"\n",
    "  4. export JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "  5. echo \"**** PATH: \" $PATH\n",
    "  6. echo \"**** SPARK_HOME: \" $SPARK_HOME\n",
    "  7. echo \"**** JAVA_HOME: \" $JAVA_HOME\n",
    "~                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-----------+\n",
      "|uid| last_name|first_name|birth_month|\n",
      "+---+----------+----------+-----------+\n",
      "|  1|    Asimov|     Issac|    January|\n",
      "|  2|     Brown|       Dan|       June|\n",
      "|  3|  Baldacci|     David|     August|\n",
      "|  4| Burroughs|Edgar Rice|  September|\n",
      "|  5|    Clarke| Arthur C.|   December|\n",
      "|  6|      Cook|     Robin|        May|\n",
      "|  7|   Dickens|   Charles|   February|\n",
      "|  8|   Forster|      E.M.|    January|\n",
      "|  9|    Lahiri|    Jhumpa|       July|\n",
      "| 10|      Rand|       Ayn|   February|\n",
      "| 11|  Salinger|      J.D.|    January|\n",
      "| 12|     Seuss|       Dr.|      March|\n",
      "| 13|    Stoker|      Bram|   November|\n",
      "| 14|     Wilde|     Oscar|    October|\n",
      "| 15|Wordsworth|   William|      April|\n",
      "+---+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#\n",
    "# use the spark.read() method to load data from a JDBC data source\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "dataframe_mysql = spark.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://127.0.0.1/pysparkdb\",\n",
    "    driver = \"org.mariadb.jdbc.Driver\",\n",
    "    dbtable = \"authors\",\n",
    "    user=\"root\",\n",
    "    password=\"g0nz07\").load()\n",
    "dataframe_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|# Essential PySpark for Scalable Data Analytics|\n",
      "+-----------------------------------------------+\n",
      "|                           <a href=\"https://...|\n",
      "|                           This is the code ...|\n",
      "|                           **A beginner's gu...|\n",
      "|                           ## What is this b...|\n",
      "|                           Apache Spark is a...|\n",
      "|                           This book covers ...|\n",
      "|                           Understand the ro...|\n",
      "|                           Gain an appreciat...|\n",
      "|                           Scale out your da...|\n",
      "|                           Build data pipeli...|\n",
      "|                           Leverage the clou...|\n",
      "|                           Explore the appli...|\n",
      "|                           Integrate your cl...|\n",
      "|                           If you feel this ...|\n",
      "|                           <a href=\"https://...|\n",
      "|                           alt=\"https://www....|\n",
      "|                           ## Instructions a...|\n",
      "|                           All of the code i...|\n",
      "|                           The code will loo...|\n",
      "|                                            ```|\n",
      "+-----------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the spark.read() function to read a CSV file. We specify the inferSchema and header options to be true. This helps Spark infer the column names and data type information by reading a sample set of data.\n",
    "retail_df = (spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .load(\"/home/nolfonzo/src/pyspark/Essential-PySpark-for-Scalable-Data-Analytics/README.md\")\n",
    "    )\n",
    "retail_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n",
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "|              origin|height|width|nChannels|mode|                data|\n",
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "|file:///home/nolf...|   279| 1480|        3|  16|[0E 54 72 0D 53 7...|\n",
      "|file:///home/nolf...|   323| 1504|        3|  16|[57 B4 E1 5A B9 E...|\n",
      "|file:///home/nolf...|   371| 1530|        3|  16|[FF FF FF FF FF F...|\n",
      "|file:///home/nolf...|   268|  763|        3|  16|[3F DD B5 3C D7 B...|\n",
      "|file:///home/nolf...|   197|  699|        3|  16|[FF FF FF FF FF F...|\n",
      "|file:///home/nolf...|   244|  617|        3|  16|[FF FF FF FF FF F...|\n",
      "|file:///home/nolf...|   224|  765|        3|  16|[FF FF FF FF FF F...|\n",
      "|file:///home/nolf...|   250|  766|        3|  16|[FF FF FF FF FF F...|\n",
      "|file:///home/nolf...|   124|  456|        3|  16|[11 C6 D6 18 CF D...|\n",
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Semi Structured storage formats\n",
    "# Examples of semi-structured data formats include CSV, XML, and JSON.\n",
    "# Not suitable for binary types\n",
    "#\n",
    "# ingest unstructured data and convert it into a structured format before storing it inside the data lake. This makes the downstream processing of data easier and more efficient.\n",
    "#\n",
    "# We load a set of images files using Spark's built-in image format; the result is a Spark DataFrame of image attributes.\n",
    "raw_df = spark.read.format(\"image\").load(\"/home/nolfonzo/src/pyspark/Essential-PySpark-for-Scalable-Data-Analytics/data/images\")\n",
    "# We use the printSchema() function to take a look at the DataFrame's schema and discover that the DataFrame has a single nested column named image with origin, height, width, nChannels etc\n",
    "raw_df.printSchema()\n",
    "# bring up the inner attributes to the top level using the image prefix with each inner attribute, such as image.origin, \n",
    "# and create a new DataFrame named image_df with all of the image's individual attributes as top-level columns.\n",
    "image_df = raw_df.select(\"image.origin\", \"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\", \"image.data\")\n",
    "image_df.show()\n",
    "# Now that we have our final DataFrame, we write it out to the data lake using the CSV format.\n",
    "image_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/home/nolfonzo/src/pyspark/images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "|Invoice|StockCode|         Description|Quantity|   InvoiceDate|Price|Customer ID|       Country|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|01/12/09 07:45| 6.95|      13085|United Kingdom|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|01/12/09 07:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|01/12/09 07:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|01/12/09 07:45|  2.1|      13085|United Kingdom|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|01/12/09 07:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    22064|PINK DOUGHNUT TRI...|      24|01/12/09 07:45| 1.65|      13085|United Kingdom|\n",
      "| 489434|    21871| SAVE THE PLANET MUG|      24|01/12/09 07:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    21523|FANCY FONT HOME S...|      10|01/12/09 07:45| 5.95|      13085|United Kingdom|\n",
      "| 489435|    22350|           CAT BOWL |      12|01/12/09 07:46| 2.55|      13085|United Kingdom|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   489434|    85048|15CM CHRISTMAS GL...|      12|01/12/09 07:45|     6.95|     13085|United Kingdom|\n",
      "|   489434|   79323P|  PINK CHERRY LIGHTS|      12|01/12/09 07:45|     6.75|     13085|United Kingdom|\n",
      "|   489434|   79323W| WHITE CHERRY LIGHTS|      12|01/12/09 07:45|     6.75|     13085|United Kingdom|\n",
      "|   489434|    22041|\"RECORD FRAME 7\"\"...|      48|01/12/09 07:45|      2.1|     13085|United Kingdom|\n",
      "|   489434|    21232|STRAWBERRY CERAMI...|      24|01/12/09 07:45|     1.25|     13085|United Kingdom|\n",
      "|   489434|    22064|PINK DOUGHNUT TRI...|      24|01/12/09 07:45|     1.65|     13085|United Kingdom|\n",
      "|   489434|    21871| SAVE THE PLANET MUG|      24|01/12/09 07:45|     1.25|     13085|United Kingdom|\n",
      "|   489434|    21523|FANCY FONT HOME S...|      10|01/12/09 07:45|     5.95|     13085|United Kingdom|\n",
      "|   489435|    22350|           CAT BOWL |      12|01/12/09 07:46|     2.55|     13085|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way to make header top-level columns\n",
    "csv_df = (spark \\\n",
    "    .read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/nolfonzo/src/pyspark/Essential-PySpark-for-Scalable-Data-Analytics/data/online_retail/online_retail_II_small.csv\") \\\n",
    ")\n",
    "csv_df.show()\n",
    "retail_df = csv_df.selectExpr(\"Invoice as InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"Price as UnitPrice\", \"`Customer ID` as CustomerID\", \"Country\")\n",
    "retail_df.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2678a815613a486355418d7a646a043051186984e97816ceb1e10bd784649dbc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
