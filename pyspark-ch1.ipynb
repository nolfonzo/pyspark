{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 20:06:57 WARN Utils: Your hostname, CarlPC resolves to a loopback address: 127.0.1.1; using 172.27.169.216 instead (on interface eth0)\n",
      "22/05/18 20:06:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nolfonzo/miniconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/05/18 20:06:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('PySpark', 7),\n",
       " ('Scalable', 3),\n",
       " ('Analytics', 1),\n",
       " ('', 53),\n",
       " ('src=\"https://static.packt-cdn.com/products/9781800568877/cover/smaller\"',\n",
       "  1),\n",
       " ('Analytics\"', 1),\n",
       " ('height=\"256px\"', 1),\n",
       " ('align=\"right\"></a>', 1),\n",
       " ('is', 10)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"/home/nolfonzo/src/Essential-PySpark-for-Scalable-Data-Analytics/README.md\")\n",
    "words = lines.flatMap(lambda s: s.split(\" \"))\n",
    "word_tuples = words.map(lambda s: (s, 1))\n",
    "word_count = word_tuples.reduceByKey(lambda x, y: x + y)\n",
    "word_count.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "linesDF = spark.read.text(\"/home/nolfonzo/src/Essential-PySpark-for-Scalable-Data-Analytics/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|# Essential PySpa...|\n",
      "|                    |\n",
      "|<a href=\"https://...|\n",
      "|                    |\n",
      "|This is the code ...|\n",
      "|                    |\n",
      "|**A beginner's gu...|\n",
      "|                    |\n",
      "|## What is this b...|\n",
      "|Apache Spark is a...|\n",
      "|                    |\n",
      "|This book covers ...|\n",
      "|Understand the ro...|\n",
      "|Gain an appreciat...|\n",
      "|Scale out your da...|\n",
      "|Build data pipeli...|\n",
      "|Leverage the clou...|\n",
      "|Explore the appli...|\n",
      "|Integrate your cl...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[#, Essential, Py...|\n",
      "|                  []|\n",
      "|[<a, href=\"https:...|\n",
      "|                  []|\n",
      "|[This, is, the, c...|\n",
      "|                  []|\n",
      "|[**A, beginner's,...|\n",
      "|                  []|\n",
      "|[##, What, is, th...|\n",
      "|[Apache, Spark, i...|\n",
      "|                  []|\n",
      "|[This, book, cove...|\n",
      "|[Understand, the,...|\n",
      "|[Gain, an, apprec...|\n",
      "|[Scale, out, your...|\n",
      "|[Build, data, pip...|\n",
      "|[Leverage, the, c...|\n",
      "|[Explore, the, ap...|\n",
      "|[Integrate, your,...|\n",
      "|                  []|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|                word|\n",
      "+--------------------+\n",
      "|                   #|\n",
      "|           Essential|\n",
      "|             PySpark|\n",
      "|                 for|\n",
      "|            Scalable|\n",
      "|                Data|\n",
      "|           Analytics|\n",
      "|                    |\n",
      "|                  <a|\n",
      "|href=\"https://www...|\n",
      "|src=\"https://stat...|\n",
      "|      alt=\"Essential|\n",
      "|             PySpark|\n",
      "|                 for|\n",
      "|            Scalable|\n",
      "|                Data|\n",
      "|          Analytics\"|\n",
      "|      height=\"256px\"|\n",
      "|  align=\"right\"></a>|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordListDf = linesDF.select(split(\"value\", \" \").alias(\"words\"))\n",
    "wordListDf.show()\n",
    "wordsDf = wordListDf.select(explode(\"words\").alias(\"word\"))\n",
    "wordsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|              lakes,|    1|\n",
      "|[copy](https://ww...|    1|\n",
      "|              today!|    1|\n",
      "|          Dashboards|    1|\n",
      "|                 API|    1|\n",
      "|                  If|    1|\n",
      "| .add('Description',|    1|\n",
      "|                used|    1|\n",
      "|         **Following|    1|\n",
      "|           analysts,|    1|\n",
      "|          practicing|    1|\n",
      "|                 PDF|    1|\n",
      "|                Data|    5|\n",
      "|             present|    1|\n",
      "|                   =|    1|\n",
      "|             helping|    1|\n",
      "|          beginner's|    1|\n",
      "|               go-to|    1|\n",
      "|         Databricks,|    1|\n",
      "|          border=\"5\"|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCountDf = wordsDf.groupBy(\"word\").count()\n",
    "wordCountDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                  If|    1|\n",
      "|         **Following|    1|\n",
      "|                   *|    2|\n",
      "|alt=\"https://www....|    1|\n",
      "|                null|    0|\n",
      "|            Leverage|    1|\n",
      "|              Apache|    1|\n",
      "|                  <a|    2|\n",
      "|                  is|    1|\n",
      "|                 All|    1|\n",
      "|                   ||    3|\n",
      "|                Gain|    1|\n",
      "|           **Sreeram|    1|\n",
      "|               Scale|    1|\n",
      "|           Integrate|    1|\n",
      "|                   )|    1|\n",
      "|             Explore|    1|\n",
      "|                With|    1|\n",
      "|                 The|    1|\n",
      "|                 ```|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists word_counts\")\n",
    "spark.sql('CREATE TABLE word_counts (word STRING) USING csv OPTIONS(\"delimiter\"=\" \") LOCATION \"/home/nolfonzo/src/Essential-PySpark-for-Scalable-Data-Analytics/README.md\"')\n",
    "\n",
    "spark.sql(\"SELECT word, COUNT(word) AS count FROM word_counts GROUP BY word\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2678a815613a486355418d7a646a043051186984e97816ceb1e10bd784649dbc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
