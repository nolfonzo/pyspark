{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "1. conda install pyspark\n",
    "\n",
    "Note: code in this notebook doesn't need a Spark installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import shutil   \n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDDs - partitions distributed across individual nodes of a cluster.\n",
    "# We use special functions called higher-order functions to operate on the RDDs and transform the RDDs according to our business logic\n",
    "#\n",
    "# loads all text files at the specified location into the cluster memory, splits them into individual lines, and returns an RDD of lines or strings.\n",
    "lines = sc.textFile(\"Essential-PySpark-for-Scalable-Data-Analytics/README.md\")\n",
    "# apply the flatMap() higher-order function to the new RDD of lines and supply it with a function that instructs it to take each line and split it based on a white space.\n",
    "# flatMmap() bundles the lambda and sends it over a network to the Worker Nodes via serialization, sent to every executor who all apply this lambda to individual RDD partitions in parallel.\n",
    "words = lines.flatMap(lambda s: s.split(\" \"))\n",
    "# use the map() function to assign a count of 1 to every individual word. This is pretty easy and definitely more intuitive compared to developing a MapReduce application using the Java programming language.\n",
    "word_tuples = words.map(lambda s: (s, 1))\n",
    "# Merge the values for each key using an associative and commutative reduce function.\n",
    "word_count = word_tuples.reduceByKey(lambda x, y: x + y)\n",
    "word_count.take(10)\n",
    "shutil.rmtree(\"/tmp/wordcount.txt\", ignore_errors=True)\n",
    "word_count.saveAsTextFile(\"/tmp/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|# Essential PySpa...|\n",
      "|                    |\n",
      "|<a href=\"https://...|\n",
      "|                    |\n",
      "|This is the code ...|\n",
      "|                    |\n",
      "|**A beginner's gu...|\n",
      "|                    |\n",
      "|## What is this b...|\n",
      "|Apache Spark is a...|\n",
      "|                    |\n",
      "|This book covers ...|\n",
      "|Understand the ro...|\n",
      "|Gain an appreciat...|\n",
      "|Scale out your da...|\n",
      "|Build data pipeli...|\n",
      "|Leverage the clou...|\n",
      "|Explore the appli...|\n",
      "|Integrate your cl...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The Spark SQL engine was added as a layer on top of the RDD API and expanded to every component of Spark and include the DataFrame API \n",
    "# DataFrames are immutable, and support actions write, count, show, and transformations read, select, where, filter, join & groupBy which return another DataFrame\n",
    "#\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import SparkSession\n",
    "#\n",
    "#creates a DataFrame of lines of StringType\n",
    "linesDF = spark.read.text(\"Essential-PySpark-for-Scalable-Data-Analytics/README.md\")\n",
    "linesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['#', 'Essential', 'PySpark', 'for', 'Scalable', 'Data', 'Analytics']),\n",
       " Row(words=['']),\n",
       " Row(words=['<a', 'href=\"https://www.packtpub.com/product/essential-pyspark-for-scalable-data-analytics/9781800568877?utm_source=github&utm_medium=repository&utm_campaign=9781800568877\"><img', 'src=\"https://static.packt-cdn.com/products/9781800568877/cover/smaller\"', 'alt=\"Essential', 'PySpark', 'for', 'Scalable', 'Data', 'Analytics\"', 'height=\"256px\"', 'align=\"right\"></a>']),\n",
       " Row(words=['']),\n",
       " Row(words=['This', 'is', 'the', 'code', 'repository', 'for', '[Essential', 'PySpark', 'for', 'Scalable', 'Data', 'Analytics](https://www.packtpub.com/product/essential-pyspark-for-scalable-data-analytics/9781800568877?utm_source=github&utm_medium=repository&utm_campaign=9781800568877),', 'published', 'by', 'Packt.'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Row(word='#'),\n",
       " Row(word='Essential'),\n",
       " Row(word='PySpark'),\n",
       " Row(word='for'),\n",
       " Row(word='Scalable')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Row(word='lakes,', count=1),\n",
       " Row(word='[copy](https://www.amazon.com/dp/1800568878)', count=1),\n",
       " Row(word='today!', count=1),\n",
       " Row(word='Dashboards', count=1),\n",
       " Row(word='API', count=1),\n",
       " Row(word='If', count=1),\n",
       " Row(word=\".add('Description',\", count=1),\n",
       " Row(word='used', count=1),\n",
       " Row(word='**Following', count=1),\n",
       " Row(word='analysts,', count=1),\n",
       " Row(word='practicing', count=1),\n",
       " Row(word='PDF', count=1),\n",
       " Row(word='Data', count=5),\n",
       " Row(word='present', count=1),\n",
       " Row(word='=', count=1),\n",
       " Row(word='helping', count=1),\n",
       " Row(word=\"beginner's\", count=1),\n",
       " Row(word='go-to', count=1),\n",
       " Row(word='Databricks,', count=1),\n",
       " Row(word='border=\"5\"', count=1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate out every line into its individual words; the result is a DataFrame with a single column, named value, which is actually a list of words.\n",
    "wordListDf = linesDF.select(split(\"value\", \" \").alias(\"words\"))\n",
    "wordListDf.take(5)\n",
    "# separate the list of words in each row out to every word on a separate row; the result is a DataFrame with a column labeled word.\n",
    "wordsDf = wordListDf.select(explode(\"words\").alias(\"word\"))\n",
    "wordsDf.take(5)\n",
    "wordCountDf = wordsDf.groupBy(\"word\").count()\n",
    "wordCountDf.take(20)\n",
    "wordCountDf.write.csv(\"/tmp/wordcounts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                  If|    1|\n",
      "|         **Following|    1|\n",
      "|                   *|    2|\n",
      "|alt=\"https://www....|    1|\n",
      "|                null|    0|\n",
      "|            Leverage|    1|\n",
      "|              Apache|    1|\n",
      "|                  <a|    2|\n",
      "|                  is|    1|\n",
      "|                 All|    1|\n",
      "|                   ||    3|\n",
      "|                Gain|    1|\n",
      "|           **Sreeram|    1|\n",
      "|               Scale|    1|\n",
      "|           Integrate|    1|\n",
      "|                   )|    1|\n",
      "|             Explore|    1|\n",
      "|                With|    1|\n",
      "|                 The|    1|\n",
      "|                 ```|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists word_counts\")\n",
    "spark.sql('CREATE TABLE word_counts (word STRING) USING csv OPTIONS(\"delimiter\"=\" \") LOCATION \"/tmp/hive\"')\n",
    "\n",
    "spark.sql(\"SELECT word, COUNT(word) AS count FROM word_counts GROUP BY word\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2678a815613a486355418d7a646a043051186984e97816ceb1e10bd784649dbc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
