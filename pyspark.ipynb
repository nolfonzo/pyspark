{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = sc.textFile('file:////usr/share/doc/python/copyright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/usr/share/doc/python/copyright\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Input path does not exist: file:/usr/share/doc/python/copyright\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22760/4205192577.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1235\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m         \"\"\"\n\u001b[1;32m-> 1237\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1224\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m         \"\"\"\n\u001b[1;32m-> 1226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1081\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \"\"\"\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/usr/share/doc/python/copyright\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Input path does not exist: file:/usr/share/doc/python/copyright\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "print(txt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list=range(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(big_list, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds = rdd.filter(lambda x: x % 2 != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 19,\n",
       " 21,\n",
       " 23,\n",
       " 25,\n",
       " 27,\n",
       " 29,\n",
       " 31,\n",
       " 33,\n",
       " 35,\n",
       " 37,\n",
       " 39,\n",
       " 41,\n",
       " 43,\n",
       " 45,\n",
       " 47,\n",
       " 49,\n",
       " 51,\n",
       " 53,\n",
       " 55,\n",
       " 57,\n",
       " 59,\n",
       " 61,\n",
       " 63,\n",
       " 65,\n",
       " 67,\n",
       " 69,\n",
       " 71,\n",
       " 73,\n",
       " 75,\n",
       " 77,\n",
       " 79,\n",
       " 81,\n",
       " 83,\n",
       " 85,\n",
       " 87,\n",
       " 89,\n",
       " 91,\n",
       " 93,\n",
       " 95,\n",
       " 97,\n",
       " 99,\n",
       " 101,\n",
       " 103,\n",
       " 105,\n",
       " 107,\n",
       " 109,\n",
       " 111,\n",
       " 113,\n",
       " 115,\n",
       " 117,\n",
       " 119,\n",
       " 121,\n",
       " 123,\n",
       " 125,\n",
       " 127,\n",
       " 129,\n",
       " 131,\n",
       " 133,\n",
       " 135,\n",
       " 137,\n",
       " 139,\n",
       " 141,\n",
       " 143,\n",
       " 145,\n",
       " 147,\n",
       " 149,\n",
       " 151,\n",
       " 153,\n",
       " 155,\n",
       " 157,\n",
       " 159,\n",
       " 161,\n",
       " 163,\n",
       " 165,\n",
       " 167,\n",
       " 169,\n",
       " 171,\n",
       " 173,\n",
       " 175,\n",
       " 177,\n",
       " 179,\n",
       " 181,\n",
       " 183,\n",
       " 185,\n",
       " 187,\n",
       " 189,\n",
       " 191,\n",
       " 193,\n",
       " 195,\n",
       " 197,\n",
       " 199,\n",
       " 201,\n",
       " 203,\n",
       " 205,\n",
       " 207,\n",
       " 209,\n",
       " 211,\n",
       " 213,\n",
       " 215,\n",
       " 217,\n",
       " 219,\n",
       " 221,\n",
       " 223,\n",
       " 225,\n",
       " 227,\n",
       " 229,\n",
       " 231,\n",
       " 233,\n",
       " 235,\n",
       " 237,\n",
       " 239,\n",
       " 241,\n",
       " 243,\n",
       " 245,\n",
       " 247,\n",
       " 249,\n",
       " 251,\n",
       " 253,\n",
       " 255,\n",
       " 257,\n",
       " 259,\n",
       " 261,\n",
       " 263,\n",
       " 265,\n",
       " 267,\n",
       " 269,\n",
       " 271,\n",
       " 273,\n",
       " 275,\n",
       " 277,\n",
       " 279,\n",
       " 281,\n",
       " 283,\n",
       " 285,\n",
       " 287,\n",
       " 289,\n",
       " 291,\n",
       " 293,\n",
       " 295,\n",
       " 297,\n",
       " 299,\n",
       " 301,\n",
       " 303,\n",
       " 305,\n",
       " 307,\n",
       " 309,\n",
       " 311,\n",
       " 313,\n",
       " 315,\n",
       " 317,\n",
       " 319,\n",
       " 321,\n",
       " 323,\n",
       " 325,\n",
       " 327,\n",
       " 329,\n",
       " 331,\n",
       " 333,\n",
       " 335,\n",
       " 337,\n",
       " 339,\n",
       " 341,\n",
       " 343,\n",
       " 345,\n",
       " 347,\n",
       " 349,\n",
       " 351,\n",
       " 353,\n",
       " 355,\n",
       " 357,\n",
       " 359,\n",
       " 361,\n",
       " 363,\n",
       " 365,\n",
       " 367,\n",
       " 369,\n",
       " 371,\n",
       " 373,\n",
       " 375,\n",
       " 377,\n",
       " 379,\n",
       " 381,\n",
       " 383,\n",
       " 385,\n",
       " 387,\n",
       " 389,\n",
       " 391,\n",
       " 393,\n",
       " 395,\n",
       " 397,\n",
       " 399,\n",
       " 401,\n",
       " 403,\n",
       " 405,\n",
       " 407,\n",
       " 409,\n",
       " 411,\n",
       " 413,\n",
       " 415,\n",
       " 417,\n",
       " 419,\n",
       " 421,\n",
       " 423,\n",
       " 425,\n",
       " 427,\n",
       " 429,\n",
       " 431,\n",
       " 433,\n",
       " 435,\n",
       " 437,\n",
       " 439,\n",
       " 441,\n",
       " 443,\n",
       " 445,\n",
       " 447,\n",
       " 449,\n",
       " 451,\n",
       " 453,\n",
       " 455,\n",
       " 457,\n",
       " 459,\n",
       " 461,\n",
       " 463,\n",
       " 465,\n",
       " 467,\n",
       " 469,\n",
       " 471,\n",
       " 473,\n",
       " 475,\n",
       " 477,\n",
       " 479,\n",
       " 481,\n",
       " 483,\n",
       " 485,\n",
       " 487,\n",
       " 489,\n",
       " 491,\n",
       " 493,\n",
       " 495,\n",
       " 497,\n",
       " 499,\n",
       " 501,\n",
       " 503,\n",
       " 505,\n",
       " 507,\n",
       " 509,\n",
       " 511,\n",
       " 513,\n",
       " 515,\n",
       " 517,\n",
       " 519,\n",
       " 521,\n",
       " 523,\n",
       " 525,\n",
       " 527,\n",
       " 529,\n",
       " 531,\n",
       " 533,\n",
       " 535,\n",
       " 537,\n",
       " 539,\n",
       " 541,\n",
       " 543,\n",
       " 545,\n",
       " 547,\n",
       " 549,\n",
       " 551,\n",
       " 553,\n",
       " 555,\n",
       " 557,\n",
       " 559,\n",
       " 561,\n",
       " 563,\n",
       " 565,\n",
       " 567,\n",
       " 569,\n",
       " 571,\n",
       " 573,\n",
       " 575,\n",
       " 577,\n",
       " 579,\n",
       " 581,\n",
       " 583,\n",
       " 585,\n",
       " 587,\n",
       " 589,\n",
       " 591,\n",
       " 593,\n",
       " 595,\n",
       " 597,\n",
       " 599,\n",
       " 601,\n",
       " 603,\n",
       " 605,\n",
       " 607,\n",
       " 609,\n",
       " 611,\n",
       " 613,\n",
       " 615,\n",
       " 617,\n",
       " 619,\n",
       " 621,\n",
       " 623,\n",
       " 625,\n",
       " 627,\n",
       " 629,\n",
       " 631,\n",
       " 633,\n",
       " 635,\n",
       " 637,\n",
       " 639,\n",
       " 641,\n",
       " 643,\n",
       " 645,\n",
       " 647,\n",
       " 649,\n",
       " 651,\n",
       " 653,\n",
       " 655,\n",
       " 657,\n",
       " 659,\n",
       " 661,\n",
       " 663,\n",
       " 665,\n",
       " 667,\n",
       " 669,\n",
       " 671,\n",
       " 673,\n",
       " 675,\n",
       " 677,\n",
       " 679,\n",
       " 681,\n",
       " 683,\n",
       " 685,\n",
       " 687,\n",
       " 689,\n",
       " 691,\n",
       " 693,\n",
       " 695,\n",
       " 697,\n",
       " 699,\n",
       " 701,\n",
       " 703,\n",
       " 705,\n",
       " 707,\n",
       " 709,\n",
       " 711,\n",
       " 713,\n",
       " 715,\n",
       " 717,\n",
       " 719,\n",
       " 721,\n",
       " 723,\n",
       " 725,\n",
       " 727,\n",
       " 729,\n",
       " 731,\n",
       " 733,\n",
       " 735,\n",
       " 737,\n",
       " 739,\n",
       " 741,\n",
       " 743,\n",
       " 745,\n",
       " 747,\n",
       " 749,\n",
       " 751,\n",
       " 753,\n",
       " 755,\n",
       " 757,\n",
       " 759,\n",
       " 761,\n",
       " 763,\n",
       " 765,\n",
       " 767,\n",
       " 769,\n",
       " 771,\n",
       " 773,\n",
       " 775,\n",
       " 777,\n",
       " 779,\n",
       " 781,\n",
       " 783,\n",
       " 785,\n",
       " 787,\n",
       " 789,\n",
       " 791,\n",
       " 793,\n",
       " 795,\n",
       " 797,\n",
       " 799,\n",
       " 801,\n",
       " 803,\n",
       " 805,\n",
       " 807,\n",
       " 809,\n",
       " 811,\n",
       " 813,\n",
       " 815,\n",
       " 817,\n",
       " 819,\n",
       " 821,\n",
       " 823,\n",
       " 825,\n",
       " 827,\n",
       " 829,\n",
       " 831,\n",
       " 833,\n",
       " 835,\n",
       " 837,\n",
       " 839,\n",
       " 841,\n",
       " 843,\n",
       " 845,\n",
       " 847,\n",
       " 849,\n",
       " 851,\n",
       " 853,\n",
       " 855,\n",
       " 857,\n",
       " 859,\n",
       " 861,\n",
       " 863,\n",
       " 865,\n",
       " 867,\n",
       " 869,\n",
       " 871,\n",
       " 873,\n",
       " 875,\n",
       " 877,\n",
       " 879,\n",
       " 881,\n",
       " 883,\n",
       " 885,\n",
       " 887,\n",
       " 889,\n",
       " 891,\n",
       " 893,\n",
       " 895,\n",
       " 897,\n",
       " 899,\n",
       " 901,\n",
       " 903,\n",
       " 905,\n",
       " 907,\n",
       " 909,\n",
       " 911,\n",
       " 913,\n",
       " 915,\n",
       " 917,\n",
       " 919,\n",
       " 921,\n",
       " 923,\n",
       " 925,\n",
       " 927,\n",
       " 929,\n",
       " 931,\n",
       " 933,\n",
       " 935,\n",
       " 937,\n",
       " 939,\n",
       " 941,\n",
       " 943,\n",
       " 945,\n",
       " 947,\n",
       " 949,\n",
       " 951,\n",
       " 953,\n",
       " 955,\n",
       " 957,\n",
       " 959,\n",
       " 961,\n",
       " 963,\n",
       " 965,\n",
       " 967,\n",
       " 969,\n",
       " 971,\n",
       " 973,\n",
       " 975,\n",
       " 977,\n",
       " 979,\n",
       " 981,\n",
       " 983,\n",
       " 985,\n",
       " 987,\n",
       " 989,\n",
       " 991,\n",
       " 993,\n",
       " 995,\n",
       " 997,\n",
       " 999,\n",
       " 1001,\n",
       " 1003,\n",
       " 1005,\n",
       " 1007,\n",
       " 1009,\n",
       " 1011,\n",
       " 1013,\n",
       " 1015,\n",
       " 1017,\n",
       " 1019,\n",
       " 1021,\n",
       " 1023,\n",
       " 1025,\n",
       " 1027,\n",
       " 1029,\n",
       " 1031,\n",
       " 1033,\n",
       " 1035,\n",
       " 1037,\n",
       " 1039,\n",
       " 1041,\n",
       " 1043,\n",
       " 1045,\n",
       " 1047,\n",
       " 1049,\n",
       " 1051,\n",
       " 1053,\n",
       " 1055,\n",
       " 1057,\n",
       " 1059,\n",
       " 1061,\n",
       " 1063,\n",
       " 1065,\n",
       " 1067,\n",
       " 1069,\n",
       " 1071,\n",
       " 1073,\n",
       " 1075,\n",
       " 1077,\n",
       " 1079,\n",
       " 1081,\n",
       " 1083,\n",
       " 1085,\n",
       " 1087,\n",
       " 1089,\n",
       " 1091,\n",
       " 1093,\n",
       " 1095,\n",
       " 1097,\n",
       " 1099,\n",
       " 1101,\n",
       " 1103,\n",
       " 1105,\n",
       " 1107,\n",
       " 1109,\n",
       " 1111,\n",
       " 1113,\n",
       " 1115,\n",
       " 1117,\n",
       " 1119,\n",
       " 1121,\n",
       " 1123,\n",
       " 1125,\n",
       " 1127,\n",
       " 1129,\n",
       " 1131,\n",
       " 1133,\n",
       " 1135,\n",
       " 1137,\n",
       " 1139,\n",
       " 1141,\n",
       " 1143,\n",
       " 1145,\n",
       " 1147,\n",
       " 1149,\n",
       " 1151,\n",
       " 1153,\n",
       " 1155,\n",
       " 1157,\n",
       " 1159,\n",
       " 1161,\n",
       " 1163,\n",
       " 1165,\n",
       " 1167,\n",
       " 1169,\n",
       " 1171,\n",
       " 1173,\n",
       " 1175,\n",
       " 1177,\n",
       " 1179,\n",
       " 1181,\n",
       " 1183,\n",
       " 1185,\n",
       " 1187,\n",
       " 1189,\n",
       " 1191,\n",
       " 1193,\n",
       " 1195,\n",
       " 1197,\n",
       " 1199,\n",
       " 1201,\n",
       " 1203,\n",
       " 1205,\n",
       " 1207,\n",
       " 1209,\n",
       " 1211,\n",
       " 1213,\n",
       " 1215,\n",
       " 1217,\n",
       " 1219,\n",
       " 1221,\n",
       " 1223,\n",
       " 1225,\n",
       " 1227,\n",
       " 1229,\n",
       " 1231,\n",
       " 1233,\n",
       " 1235,\n",
       " 1237,\n",
       " 1239,\n",
       " 1241,\n",
       " 1243,\n",
       " 1245,\n",
       " 1247,\n",
       " 1249,\n",
       " 1251,\n",
       " 1253,\n",
       " 1255,\n",
       " 1257,\n",
       " 1259,\n",
       " 1261,\n",
       " 1263,\n",
       " 1265,\n",
       " 1267,\n",
       " 1269,\n",
       " 1271,\n",
       " 1273,\n",
       " 1275,\n",
       " 1277,\n",
       " 1279,\n",
       " 1281,\n",
       " 1283,\n",
       " 1285,\n",
       " 1287,\n",
       " 1289,\n",
       " 1291,\n",
       " 1293,\n",
       " 1295,\n",
       " 1297,\n",
       " 1299,\n",
       " 1301,\n",
       " 1303,\n",
       " 1305,\n",
       " 1307,\n",
       " 1309,\n",
       " 1311,\n",
       " 1313,\n",
       " 1315,\n",
       " 1317,\n",
       " 1319,\n",
       " 1321,\n",
       " 1323,\n",
       " 1325,\n",
       " 1327,\n",
       " 1329,\n",
       " 1331,\n",
       " 1333,\n",
       " 1335,\n",
       " 1337,\n",
       " 1339,\n",
       " 1341,\n",
       " 1343,\n",
       " 1345,\n",
       " 1347,\n",
       " 1349,\n",
       " 1351,\n",
       " 1353,\n",
       " 1355,\n",
       " 1357,\n",
       " 1359,\n",
       " 1361,\n",
       " 1363,\n",
       " 1365,\n",
       " 1367,\n",
       " 1369,\n",
       " 1371,\n",
       " 1373,\n",
       " 1375,\n",
       " 1377,\n",
       " 1379,\n",
       " 1381,\n",
       " 1383,\n",
       " 1385,\n",
       " 1387,\n",
       " 1389,\n",
       " 1391,\n",
       " 1393,\n",
       " 1395,\n",
       " 1397,\n",
       " 1399,\n",
       " 1401,\n",
       " 1403,\n",
       " 1405,\n",
       " 1407,\n",
       " 1409,\n",
       " 1411,\n",
       " 1413,\n",
       " 1415,\n",
       " 1417,\n",
       " 1419,\n",
       " 1421,\n",
       " 1423,\n",
       " 1425,\n",
       " 1427,\n",
       " 1429,\n",
       " 1431,\n",
       " 1433,\n",
       " 1435,\n",
       " 1437,\n",
       " 1439,\n",
       " 1441,\n",
       " 1443,\n",
       " 1445,\n",
       " 1447,\n",
       " 1449,\n",
       " 1451,\n",
       " 1453,\n",
       " 1455,\n",
       " 1457,\n",
       " 1459,\n",
       " 1461,\n",
       " 1463,\n",
       " 1465,\n",
       " 1467,\n",
       " 1469,\n",
       " 1471,\n",
       " 1473,\n",
       " 1475,\n",
       " 1477,\n",
       " 1479,\n",
       " 1481,\n",
       " 1483,\n",
       " 1485,\n",
       " 1487,\n",
       " 1489,\n",
       " 1491,\n",
       " 1493,\n",
       " 1495,\n",
       " 1497,\n",
       " 1499,\n",
       " 1501,\n",
       " 1503,\n",
       " 1505,\n",
       " 1507,\n",
       " 1509,\n",
       " 1511,\n",
       " 1513,\n",
       " 1515,\n",
       " 1517,\n",
       " 1519,\n",
       " 1521,\n",
       " 1523,\n",
       " 1525,\n",
       " 1527,\n",
       " 1529,\n",
       " 1531,\n",
       " 1533,\n",
       " 1535,\n",
       " 1537,\n",
       " 1539,\n",
       " 1541,\n",
       " 1543,\n",
       " 1545,\n",
       " 1547,\n",
       " 1549,\n",
       " 1551,\n",
       " 1553,\n",
       " 1555,\n",
       " 1557,\n",
       " 1559,\n",
       " 1561,\n",
       " 1563,\n",
       " 1565,\n",
       " 1567,\n",
       " 1569,\n",
       " 1571,\n",
       " 1573,\n",
       " 1575,\n",
       " 1577,\n",
       " 1579,\n",
       " 1581,\n",
       " 1583,\n",
       " 1585,\n",
       " 1587,\n",
       " 1589,\n",
       " 1591,\n",
       " 1593,\n",
       " 1595,\n",
       " 1597,\n",
       " 1599,\n",
       " 1601,\n",
       " 1603,\n",
       " 1605,\n",
       " 1607,\n",
       " 1609,\n",
       " 1611,\n",
       " 1613,\n",
       " 1615,\n",
       " 1617,\n",
       " 1619,\n",
       " 1621,\n",
       " 1623,\n",
       " 1625,\n",
       " 1627,\n",
       " 1629,\n",
       " 1631,\n",
       " 1633,\n",
       " 1635,\n",
       " 1637,\n",
       " 1639,\n",
       " 1641,\n",
       " 1643,\n",
       " 1645,\n",
       " 1647,\n",
       " 1649,\n",
       " 1651,\n",
       " 1653,\n",
       " 1655,\n",
       " 1657,\n",
       " 1659,\n",
       " 1661,\n",
       " 1663,\n",
       " 1665,\n",
       " 1667,\n",
       " 1669,\n",
       " 1671,\n",
       " 1673,\n",
       " 1675,\n",
       " 1677,\n",
       " 1679,\n",
       " 1681,\n",
       " 1683,\n",
       " 1685,\n",
       " 1687,\n",
       " 1689,\n",
       " 1691,\n",
       " 1693,\n",
       " 1695,\n",
       " 1697,\n",
       " 1699,\n",
       " 1701,\n",
       " 1703,\n",
       " 1705,\n",
       " 1707,\n",
       " 1709,\n",
       " 1711,\n",
       " 1713,\n",
       " 1715,\n",
       " 1717,\n",
       " 1719,\n",
       " 1721,\n",
       " 1723,\n",
       " 1725,\n",
       " 1727,\n",
       " 1729,\n",
       " 1731,\n",
       " 1733,\n",
       " 1735,\n",
       " 1737,\n",
       " 1739,\n",
       " 1741,\n",
       " 1743,\n",
       " 1745,\n",
       " 1747,\n",
       " 1749,\n",
       " 1751,\n",
       " 1753,\n",
       " 1755,\n",
       " 1757,\n",
       " 1759,\n",
       " 1761,\n",
       " 1763,\n",
       " 1765,\n",
       " 1767,\n",
       " 1769,\n",
       " 1771,\n",
       " 1773,\n",
       " 1775,\n",
       " 1777,\n",
       " 1779,\n",
       " 1781,\n",
       " 1783,\n",
       " 1785,\n",
       " 1787,\n",
       " 1789,\n",
       " 1791,\n",
       " 1793,\n",
       " 1795,\n",
       " 1797,\n",
       " 1799,\n",
       " 1801,\n",
       " 1803,\n",
       " 1805,\n",
       " 1807,\n",
       " 1809,\n",
       " 1811,\n",
       " 1813,\n",
       " 1815,\n",
       " 1817,\n",
       " 1819,\n",
       " 1821,\n",
       " 1823,\n",
       " 1825,\n",
       " 1827,\n",
       " 1829,\n",
       " 1831,\n",
       " 1833,\n",
       " 1835,\n",
       " 1837,\n",
       " 1839,\n",
       " 1841,\n",
       " 1843,\n",
       " 1845,\n",
       " 1847,\n",
       " 1849,\n",
       " 1851,\n",
       " 1853,\n",
       " 1855,\n",
       " 1857,\n",
       " 1859,\n",
       " 1861,\n",
       " 1863,\n",
       " 1865,\n",
       " 1867,\n",
       " 1869,\n",
       " 1871,\n",
       " 1873,\n",
       " 1875,\n",
       " 1877,\n",
       " 1879,\n",
       " 1881,\n",
       " 1883,\n",
       " 1885,\n",
       " 1887,\n",
       " 1889,\n",
       " 1891,\n",
       " 1893,\n",
       " 1895,\n",
       " 1897,\n",
       " 1899,\n",
       " 1901,\n",
       " 1903,\n",
       " 1905,\n",
       " 1907,\n",
       " 1909,\n",
       " 1911,\n",
       " 1913,\n",
       " 1915,\n",
       " 1917,\n",
       " 1919,\n",
       " 1921,\n",
       " 1923,\n",
       " 1925,\n",
       " 1927,\n",
       " 1929,\n",
       " 1931,\n",
       " 1933,\n",
       " 1935,\n",
       " 1937,\n",
       " 1939,\n",
       " 1941,\n",
       " 1943,\n",
       " 1945,\n",
       " 1947,\n",
       " 1949,\n",
       " 1951,\n",
       " 1953,\n",
       " 1955,\n",
       " 1957,\n",
       " 1959,\n",
       " 1961,\n",
       " 1963,\n",
       " 1965,\n",
       " 1967,\n",
       " 1969,\n",
       " 1971,\n",
       " 1973,\n",
       " 1975,\n",
       " 1977,\n",
       " 1979,\n",
       " 1981,\n",
       " 1983,\n",
       " 1985,\n",
       " 1987,\n",
       " 1989,\n",
       " 1991,\n",
       " 1993,\n",
       " 1995,\n",
       " 1997,\n",
       " 1999,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"C://Users//nolfonzo//Spark//README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C://Users//nolfonzo//Spark//README.md MapPartitionsRDD[13] at textFile at <unknown>:0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda s: s.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tuples = words.map(lambda s: (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 1),\n",
       " ('', 1),\n",
       " ('Spark', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('unified', 1),\n",
       " ('analytics', 1),\n",
       " ('engine', 1),\n",
       " ('for', 1),\n",
       " ('large-scale', 1),\n",
       " ('data', 1),\n",
       " ('processing.', 1),\n",
       " ('It', 1),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 1),\n",
       " ('Scala,', 1),\n",
       " ('Java,', 1),\n",
       " ('Python,', 1),\n",
       " ('and', 1),\n",
       " ('R,', 1),\n",
       " ('and', 1),\n",
       " ('an', 1),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('that', 1),\n",
       " ('supports', 1),\n",
       " ('general', 1),\n",
       " ('computation', 1),\n",
       " ('graphs', 1),\n",
       " ('for', 1),\n",
       " ('data', 1),\n",
       " ('analysis.', 1),\n",
       " ('It', 1),\n",
       " ('also', 1),\n",
       " ('supports', 1),\n",
       " ('a', 1),\n",
       " ('rich', 1),\n",
       " ('set', 1),\n",
       " ('of', 1),\n",
       " ('higher-level', 1),\n",
       " ('tools', 1),\n",
       " ('including', 1),\n",
       " ('Spark', 1),\n",
       " ('SQL', 1),\n",
       " ('for', 1),\n",
       " ('SQL', 1),\n",
       " ('and', 1),\n",
       " ('DataFrames,', 1),\n",
       " ('MLlib', 1),\n",
       " ('for', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('for', 1),\n",
       " ('graph', 1),\n",
       " ('processing,', 1),\n",
       " ('and', 1),\n",
       " ('Structured', 1),\n",
       " ('Streaming', 1),\n",
       " ('for', 1),\n",
       " ('stream', 1),\n",
       " ('processing.', 1),\n",
       " ('', 1),\n",
       " ('<https://spark.apache.org/>', 1),\n",
       " ('', 1),\n",
       " ('[![GitHub', 1),\n",
       " ('Action', 1),\n",
       " ('Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster)',\n",
       "  1),\n",
       " ('[![Jenkins', 1),\n",
       " ('Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2)',\n",
       "  1),\n",
       " ('[![AppVeyor', 1),\n",
       " ('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       "  1),\n",
       " ('[![PySpark', 1),\n",
       " ('Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Online', 1),\n",
       " ('Documentation', 1),\n",
       " ('', 1),\n",
       " ('You', 1),\n",
       " ('can', 1),\n",
       " ('find', 1),\n",
       " ('the', 1),\n",
       " ('latest', 1),\n",
       " ('Spark', 1),\n",
       " ('documentation,', 1),\n",
       " ('including', 1),\n",
       " ('a', 1),\n",
       " ('programming', 1),\n",
       " ('guide,', 1),\n",
       " ('on', 1),\n",
       " ('the', 1),\n",
       " ('[project', 1),\n",
       " ('web', 1),\n",
       " ('page](https://spark.apache.org/documentation.html).', 1),\n",
       " ('This', 1),\n",
       " ('README', 1),\n",
       " ('file', 1),\n",
       " ('only', 1),\n",
       " ('contains', 1),\n",
       " ('basic', 1),\n",
       " ('setup', 1),\n",
       " ('instructions.', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Building', 1),\n",
       " ('Spark', 1),\n",
       " ('', 1),\n",
       " ('Spark', 1),\n",
       " ('is', 1),\n",
       " ('built', 1),\n",
       " ('using', 1),\n",
       " ('[Apache', 1),\n",
       " ('Maven](https://maven.apache.org/).', 1),\n",
       " ('To', 1),\n",
       " ('build', 1),\n",
       " ('Spark', 1),\n",
       " ('and', 1),\n",
       " ('its', 1),\n",
       " ('example', 1),\n",
       " ('programs,', 1),\n",
       " ('run:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('./build/mvn', 1),\n",
       " ('-DskipTests', 1),\n",
       " ('clean', 1),\n",
       " ('package', 1),\n",
       " ('', 1),\n",
       " ('(You', 1),\n",
       " ('do', 1),\n",
       " ('not', 1),\n",
       " ('need', 1),\n",
       " ('to', 1),\n",
       " ('do', 1),\n",
       " ('this', 1),\n",
       " ('if', 1),\n",
       " ('you', 1),\n",
       " ('downloaded', 1),\n",
       " ('a', 1),\n",
       " ('pre-built', 1),\n",
       " ('package.)', 1),\n",
       " ('', 1),\n",
       " ('More', 1),\n",
       " ('detailed', 1),\n",
       " ('documentation', 1),\n",
       " ('is', 1),\n",
       " ('available', 1),\n",
       " ('from', 1),\n",
       " ('the', 1),\n",
       " ('project', 1),\n",
       " ('site,', 1),\n",
       " ('at', 1),\n",
       " ('[\"Building', 1),\n",
       " ('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('', 1),\n",
       " ('For', 1),\n",
       " ('general', 1),\n",
       " ('development', 1),\n",
       " ('tips,', 1),\n",
       " ('including', 1),\n",
       " ('info', 1),\n",
       " ('on', 1),\n",
       " ('developing', 1),\n",
       " ('Spark', 1),\n",
       " ('using', 1),\n",
       " ('an', 1),\n",
       " ('IDE,', 1),\n",
       " ('see', 1),\n",
       " ('[\"Useful', 1),\n",
       " ('Developer', 1),\n",
       " ('Tools\"](https://spark.apache.org/developer-tools.html).', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Interactive', 1),\n",
       " ('Scala', 1),\n",
       " ('Shell', 1),\n",
       " ('', 1),\n",
       " ('The', 1),\n",
       " ('easiest', 1),\n",
       " ('way', 1),\n",
       " ('to', 1),\n",
       " ('start', 1),\n",
       " ('using', 1),\n",
       " ('Spark', 1),\n",
       " ('is', 1),\n",
       " ('through', 1),\n",
       " ('the', 1),\n",
       " ('Scala', 1),\n",
       " ('shell:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('./bin/spark-shell', 1),\n",
       " ('', 1),\n",
       " ('Try', 1),\n",
       " ('the', 1),\n",
       " ('following', 1),\n",
       " ('command,', 1),\n",
       " ('which', 1),\n",
       " ('should', 1),\n",
       " ('return', 1),\n",
       " ('1,000,000,000:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('scala>', 1),\n",
       " ('spark.range(1000', 1),\n",
       " ('*', 1),\n",
       " ('1000', 1),\n",
       " ('*', 1),\n",
       " ('1000).count()', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Interactive', 1),\n",
       " ('Python', 1),\n",
       " ('Shell', 1),\n",
       " ('', 1),\n",
       " ('Alternatively,', 1),\n",
       " ('if', 1),\n",
       " ('you', 1),\n",
       " ('prefer', 1),\n",
       " ('Python,', 1),\n",
       " ('you', 1),\n",
       " ('can', 1),\n",
       " ('use', 1),\n",
       " ('the', 1),\n",
       " ('Python', 1),\n",
       " ('shell:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('./bin/pyspark', 1),\n",
       " ('', 1),\n",
       " ('And', 1),\n",
       " ('run', 1),\n",
       " ('the', 1),\n",
       " ('following', 1),\n",
       " ('command,', 1),\n",
       " ('which', 1),\n",
       " ('should', 1),\n",
       " ('also', 1),\n",
       " ('return', 1),\n",
       " ('1,000,000,000:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('>>>', 1),\n",
       " ('spark.range(1000', 1),\n",
       " ('*', 1),\n",
       " ('1000', 1),\n",
       " ('*', 1),\n",
       " ('1000).count()', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Example', 1),\n",
       " ('Programs', 1),\n",
       " ('', 1),\n",
       " ('Spark', 1),\n",
       " ('also', 1),\n",
       " ('comes', 1),\n",
       " ('with', 1),\n",
       " ('several', 1),\n",
       " ('sample', 1),\n",
       " ('programs', 1),\n",
       " ('in', 1),\n",
       " ('the', 1),\n",
       " ('`examples`', 1),\n",
       " ('directory.', 1),\n",
       " ('To', 1),\n",
       " ('run', 1),\n",
       " ('one', 1),\n",
       " ('of', 1),\n",
       " ('them,', 1),\n",
       " ('use', 1),\n",
       " ('`./bin/run-example', 1),\n",
       " ('<class>', 1),\n",
       " ('[params]`.', 1),\n",
       " ('For', 1),\n",
       " ('example:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('./bin/run-example', 1),\n",
       " ('SparkPi', 1),\n",
       " ('', 1),\n",
       " ('will', 1),\n",
       " ('run', 1),\n",
       " ('the', 1),\n",
       " ('Pi', 1),\n",
       " ('example', 1),\n",
       " ('locally.', 1),\n",
       " ('', 1),\n",
       " ('You', 1),\n",
       " ('can', 1),\n",
       " ('set', 1),\n",
       " ('the', 1),\n",
       " ('MASTER', 1),\n",
       " ('environment', 1),\n",
       " ('variable', 1),\n",
       " ('when', 1),\n",
       " ('running', 1),\n",
       " ('examples', 1),\n",
       " ('to', 1),\n",
       " ('submit', 1),\n",
       " ('examples', 1),\n",
       " ('to', 1),\n",
       " ('a', 1),\n",
       " ('cluster.', 1),\n",
       " ('This', 1),\n",
       " ('can', 1),\n",
       " ('be', 1),\n",
       " ('a', 1),\n",
       " ('mesos://', 1),\n",
       " ('or', 1),\n",
       " ('spark://', 1),\n",
       " ('URL,', 1),\n",
       " ('\"yarn\"', 1),\n",
       " ('to', 1),\n",
       " ('run', 1),\n",
       " ('on', 1),\n",
       " ('YARN,', 1),\n",
       " ('and', 1),\n",
       " ('\"local\"', 1),\n",
       " ('to', 1),\n",
       " ('run', 1),\n",
       " ('locally', 1),\n",
       " ('with', 1),\n",
       " ('one', 1),\n",
       " ('thread,', 1),\n",
       " ('or', 1),\n",
       " ('\"local[N]\"', 1),\n",
       " ('to', 1),\n",
       " ('run', 1),\n",
       " ('locally', 1),\n",
       " ('with', 1),\n",
       " ('N', 1),\n",
       " ('threads.', 1),\n",
       " ('You', 1),\n",
       " ('can', 1),\n",
       " ('also', 1),\n",
       " ('use', 1),\n",
       " ('an', 1),\n",
       " ('abbreviated', 1),\n",
       " ('class', 1),\n",
       " ('name', 1),\n",
       " ('if', 1),\n",
       " ('the', 1),\n",
       " ('class', 1),\n",
       " ('is', 1),\n",
       " ('in', 1),\n",
       " ('the', 1),\n",
       " ('`examples`', 1),\n",
       " ('package.', 1),\n",
       " ('For', 1),\n",
       " ('instance:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('MASTER=spark://host:7077', 1),\n",
       " ('./bin/run-example', 1),\n",
       " ('SparkPi', 1),\n",
       " ('', 1),\n",
       " ('Many', 1),\n",
       " ('of', 1),\n",
       " ('the', 1),\n",
       " ('example', 1),\n",
       " ('programs', 1),\n",
       " ('print', 1),\n",
       " ('usage', 1),\n",
       " ('help', 1),\n",
       " ('if', 1),\n",
       " ('no', 1),\n",
       " ('params', 1),\n",
       " ('are', 1),\n",
       " ('given.', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Running', 1),\n",
       " ('Tests', 1),\n",
       " ('', 1),\n",
       " ('Testing', 1),\n",
       " ('first', 1),\n",
       " ('requires', 1),\n",
       " ('[building', 1),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Once', 1),\n",
       " ('Spark', 1),\n",
       " ('is', 1),\n",
       " ('built,', 1),\n",
       " ('tests', 1),\n",
       " ('can', 1),\n",
       " ('be', 1),\n",
       " ('run', 1),\n",
       " ('using:', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('./dev/run-tests', 1),\n",
       " ('', 1),\n",
       " ('Please', 1),\n",
       " ('see', 1),\n",
       " ('the', 1),\n",
       " ('guidance', 1),\n",
       " ('on', 1),\n",
       " ('how', 1),\n",
       " ('to', 1),\n",
       " ('[run', 1),\n",
       " ('tests', 1),\n",
       " ('for', 1),\n",
       " ('a', 1),\n",
       " ('module,', 1),\n",
       " ('or', 1),\n",
       " ('individual', 1),\n",
       " ('tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('There', 1),\n",
       " ('is', 1),\n",
       " ('also', 1),\n",
       " ('a', 1),\n",
       " ('Kubernetes', 1),\n",
       " ('integration', 1),\n",
       " ('test,', 1),\n",
       " ('see', 1),\n",
       " ('resource-managers/kubernetes/integration-tests/README.md', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('A', 1),\n",
       " ('Note', 1),\n",
       " ('About', 1),\n",
       " ('Hadoop', 1),\n",
       " ('Versions', 1),\n",
       " ('', 1),\n",
       " ('Spark', 1),\n",
       " ('uses', 1),\n",
       " ('the', 1),\n",
       " ('Hadoop', 1),\n",
       " ('core', 1),\n",
       " ('library', 1),\n",
       " ('to', 1),\n",
       " ('talk', 1),\n",
       " ('to', 1),\n",
       " ('HDFS', 1),\n",
       " ('and', 1),\n",
       " ('other', 1),\n",
       " ('Hadoop-supported', 1),\n",
       " ('storage', 1),\n",
       " ('systems.', 1),\n",
       " ('Because', 1),\n",
       " ('the', 1),\n",
       " ('protocols', 1),\n",
       " ('have', 1),\n",
       " ('changed', 1),\n",
       " ('in', 1),\n",
       " ('different', 1),\n",
       " ('versions', 1),\n",
       " ('of', 1),\n",
       " ('Hadoop,', 1),\n",
       " ('you', 1),\n",
       " ('must', 1),\n",
       " ('build', 1),\n",
       " ('Spark', 1),\n",
       " ('against', 1),\n",
       " ('the', 1),\n",
       " ('same', 1),\n",
       " ('version', 1),\n",
       " ('that', 1),\n",
       " ('your', 1),\n",
       " ('cluster', 1),\n",
       " ('runs.', 1),\n",
       " ('', 1),\n",
       " ('Please', 1),\n",
       " ('refer', 1),\n",
       " ('to', 1),\n",
       " ('the', 1),\n",
       " ('build', 1),\n",
       " ('documentation', 1),\n",
       " ('at', 1),\n",
       " ('[\"Specifying', 1),\n",
       " ('the', 1),\n",
       " ('Hadoop', 1),\n",
       " ('Version', 1),\n",
       " ('and', 1),\n",
       " ('Enabling', 1),\n",
       " ('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       "  1),\n",
       " ('for', 1),\n",
       " ('detailed', 1),\n",
       " ('guidance', 1),\n",
       " ('on', 1),\n",
       " ('building', 1),\n",
       " ('for', 1),\n",
       " ('a', 1),\n",
       " ('particular', 1),\n",
       " ('distribution', 1),\n",
       " ('of', 1),\n",
       " ('Hadoop,', 1),\n",
       " ('including', 1),\n",
       " ('building', 1),\n",
       " ('for', 1),\n",
       " ('particular', 1),\n",
       " ('Hive', 1),\n",
       " ('and', 1),\n",
       " ('Hive', 1),\n",
       " ('Thriftserver', 1),\n",
       " ('distributions.', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Configuration', 1),\n",
       " ('', 1),\n",
       " ('Please', 1),\n",
       " ('refer', 1),\n",
       " ('to', 1),\n",
       " ('the', 1),\n",
       " ('[Configuration', 1),\n",
       " ('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1),\n",
       " ('in', 1),\n",
       " ('the', 1),\n",
       " ('online', 1),\n",
       " ('documentation', 1),\n",
       " ('for', 1),\n",
       " ('an', 1),\n",
       " ('overview', 1),\n",
       " ('on', 1),\n",
       " ('how', 1),\n",
       " ('to', 1),\n",
       " ('configure', 1),\n",
       " ('Spark.', 1),\n",
       " ('', 1),\n",
       " ('##', 1),\n",
       " ('Contributing', 1),\n",
       " ('', 1),\n",
       " ('Please', 1),\n",
       " ('review', 1),\n",
       " ('the', 1),\n",
       " ('[Contribution', 1),\n",
       " ('to', 1),\n",
       " ('Spark', 1),\n",
       " ('guide](https://spark.apache.org/contributing.html)', 1),\n",
       " ('for', 1),\n",
       " ('information', 1),\n",
       " ('on', 1),\n",
       " ('how', 1),\n",
       " ('to', 1),\n",
       " ('get', 1),\n",
       " ('started', 1),\n",
       " ('contributing', 1),\n",
       " ('to', 1),\n",
       " ('the', 1),\n",
       " ('project.', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'take' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22760/1751795137.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'take' is not defined"
     ]
    }
   ],
   "source": [
    "take(word_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o55.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/nolfonzo/Spark/README.md\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Input path does not exist: file:/C:/Users/nolfonzo/Spark/README.md\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22760/1874436643.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_tuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \"\"\"\n\u001b[0;32m   1534\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1536\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o55.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/nolfonzo/Spark/README.md\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Input path does not exist: file:/C:/Users/nolfonzo/Spark/README.md\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "word_tuples.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesDF = spark.read.text(\"C://Users//nolfonzo//Spark//README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n"
     ]
    }
   ],
   "source": [
    "print(linesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|                    |\n",
      "|Spark is a unifie...|\n",
      "|high-level APIs i...|\n",
      "|supports general ...|\n",
      "|rich set of highe...|\n",
      "|MLlib for machine...|\n",
      "|and Structured St...|\n",
      "|                    |\n",
      "|<https://spark.ap...|\n",
      "|                    |\n",
      "|[![GitHub Action ...|\n",
      "|[![Jenkins Build]...|\n",
      "|[![AppVeyor Build...|\n",
      "|[![PySpark Covera...|\n",
      "|                    |\n",
      "|                    |\n",
      "|## Online Documen...|\n",
      "|                    |\n",
      "|You can find the ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordListDf = linesDF.select(split(\"value\", \" \").alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|  [#, Apache, Spark]|\n",
      "|                  []|\n",
      "|[Spark, is, a, un...|\n",
      "|[high-level, APIs...|\n",
      "|[supports, genera...|\n",
      "|[rich, set, of, h...|\n",
      "|[MLlib, for, mach...|\n",
      "|[and, Structured,...|\n",
      "|                  []|\n",
      "|[<https://spark.a...|\n",
      "|                  []|\n",
      "|[[![GitHub, Actio...|\n",
      "|[[![Jenkins, Buil...|\n",
      "|[[![AppVeyor, Bui...|\n",
      "|[[![PySpark, Cove...|\n",
      "|                  []|\n",
      "|                  []|\n",
      "|[##, Online, Docu...|\n",
      "|                  []|\n",
      "|[You, can, find, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordListDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsDf = wordListDf.select(explode(\"words\").alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       word|\n",
      "+-----------+\n",
      "|          #|\n",
      "|     Apache|\n",
      "|      Spark|\n",
      "|           |\n",
      "|      Spark|\n",
      "|         is|\n",
      "|          a|\n",
      "|    unified|\n",
      "|  analytics|\n",
      "|     engine|\n",
      "|        for|\n",
      "|large-scale|\n",
      "|       data|\n",
      "|processing.|\n",
      "|         It|\n",
      "|   provides|\n",
      "| high-level|\n",
      "|       APIs|\n",
      "|         in|\n",
      "|     Scala,|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountDf = wordsDf.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           word|count|\n",
      "+---------------+-----+\n",
      "|     [![PySpark|    1|\n",
      "|         online|    1|\n",
      "|         graphs|    1|\n",
      "|     [\"Building|    1|\n",
      "|  documentation|    3|\n",
      "|       command,|    2|\n",
      "|    abbreviated|    1|\n",
      "|       overview|    1|\n",
      "|           rich|    1|\n",
      "|            set|    2|\n",
      "|    -DskipTests|    1|\n",
      "| 1,000,000,000:|    2|\n",
      "|           name|    1|\n",
      "|   [\"Specifying|    1|\n",
      "|         stream|    1|\n",
      "|           run:|    1|\n",
      "|            not|    1|\n",
      "|       programs|    2|\n",
      "|          tests|    2|\n",
      "|./dev/run-tests|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCountDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22760/2396355227.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\nolfonzo\\AppData\\Local\\Temp/ipykernel_22760/2396355227.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    CREATE TABLE word_counts (word STRING)\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CREATE TABLE word_counts (word STRING)\n",
    "USING csv\n",
    "OPTIONS(\"delimiter\"=\" \")\n",
    "LOCATION \"/databricks-datasets/README.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE word_counts (word STRING) USING csv OPTIONS('delimiter'=' ') LOCATION '/databricks-datasets/README.md'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'word' given input columns: []; line 1 pos 7;\n'Project ['word, 'COUNT('word) AS count#42]\n+- OneRowRelation\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22760/1021091173.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT word, COUNT(word) AS count\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'word' given input columns: []; line 1 pos 7;\n'Project ['word, 'COUNT('word) AS count#42]\n+- OneRowRelation\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT word, COUNT(word) AS count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table default.word_counts already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22760/941910484.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CREATE TABLE word_counts (word STRING) USING csv OPTIONS(\"delimiter\"=\" \") LOCATION \"/databricks-datasets/README.md\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nolfonzo\\.conda\\envs\\python3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Table default.word_counts already exists."
     ]
    }
   ],
   "source": [
    "spark.sql('CREATE TABLE word_counts (word STRING) USING csv OPTIONS(\"delimiter\"=\" \") LOCATION \"/databricks-datasets/README.md\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = spark.sql(\"SELECT word, COUNT(word) AS count FROM word_counts GROUP BY word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table word_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE TABLE word_counts (word STRING) USING csv OPTIONS(\"delimiter\"=\" \") LOCATION \"C://Users//nolfonzo//Spark//README.md\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|          [![PySpark|    1|\n",
      "|          [\"Building|    1|\n",
      "|                rich|    1|\n",
      "|        [\"Specifying|    1|\n",
      "|                will|    1|\n",
      "|                [run|    1|\n",
      "|      Alternatively,|    1|\n",
      "|               MLlib|    1|\n",
      "|                 can|    2|\n",
      "|                 for|    2|\n",
      "|                null|    0|\n",
      "|                  in|    1|\n",
      "|            building|    1|\n",
      "|             locally|    1|\n",
      "|         [![AppVeyor|    1|\n",
      "|<https://spark.ap...|    1|\n",
      "|                 You|    2|\n",
      "|                  To|    2|\n",
      "|          [![Jenkins|    1|\n",
      "|                More|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT word, COUNT(word) AS count FROM word_counts GROUP BY word\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('CREATE TABLE word_counts2 (word STRING) USING csv OPTIONS(\"delimiter\"=\" \") LOCATION \"C://Users//nolfonzo//Spark//README.md\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                word|\n",
      "+--------------------+\n",
      "|                   #|\n",
      "|               Spark|\n",
      "|          high-level|\n",
      "|            supports|\n",
      "|                rich|\n",
      "|               MLlib|\n",
      "|                 and|\n",
      "|<https://spark.ap...|\n",
      "|           [![GitHub|\n",
      "|          [![Jenkins|\n",
      "|         [![AppVeyor|\n",
      "|          [![PySpark|\n",
      "|                  ##|\n",
      "|                 You|\n",
      "|              guide,|\n",
      "|                This|\n",
      "|                  ##|\n",
      "|               Spark|\n",
      "|                  To|\n",
      "|                null|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from word_counts2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42bd7e772f06a35056827be8e80a6eec0d7a0312cbaa23f37cb1640791bda56e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('python3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
