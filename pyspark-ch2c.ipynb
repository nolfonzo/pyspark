{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 pyspark-shell'\n",
    "import pyspark;\n",
    "sc = pyspark.SparkContext.getOrCreate();\n",
    "from pyspark.sql import SparkSession;\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "eventSchema = (StructType()\n",
    "  .add('InvoiceNo', StringType())\n",
    "  .add('StockCode', StringType())\n",
    "  .add('Description', StringType())\n",
    "  .add('Quantity', IntegerType())\n",
    "  .add('InvoiceDate', StringType())\n",
    "  .add('UnitPrice', DoubleType())\n",
    "  .add('CustomerID', IntegerType())\n",
    "  .add('Country', StringType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"retail-events\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 20:53:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-------+\n",
      "| key|  value|\n",
      "+----+-------+\n",
      "|null|testing|\n",
      "|null|   test|\n",
      "|null|   test|\n",
      "|null|   hiya|\n",
      "|null|   more|\n",
      "|null|   test|\n",
      "|null|   test|\n",
      "|null|   test|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "| key|value|\n",
      "+----+-----+\n",
      "|null| tttt|\n",
      "+----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+---------+\n",
      "| key|    value|\n",
      "+----+---------+\n",
      "|null|yyyyyyyyy|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/HDFS/dir\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, to_date\n",
    "retail_df = (kafka_df\\\n",
    "   .select(from_json(col(\"value\").cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\"))\\\n",
    "   .select(\"message.*\", \"EventTime\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 21:12:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f83efb6ef10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_path = \"/tmp/data-lake/retail_events.parquet\"\n",
    "(retail_df\n",
    "    .withColumn(\"EventDate\", to_date(retail_df.EventTime))\n",
    "    .writeStream\n",
    "    .format('parquet')\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True)\n",
    "    .option('checkpointLocation', base_path + '/_checkpoint')\n",
    "    .start(base_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.parquet(\"/tmp/data-lake/retail_events.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2678a815613a486355418d7a646a043051186984e97816ceb1e10bd784649dbc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
