{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incremental Data Loads using Kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 pyspark-shell'\n",
    "import pyspark;\n",
    "sc = pyspark.SparkContext.getOrCreate();\n",
    "from pyspark.sql import SparkSession;\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a stream of events from a Kafka topic, called retail-events\n",
    "# The events in a Kafka topic follow a key-value pattern. This means that our actual data is encoded within a JSON object in the value column that we need to extract\n",
    "#       \n",
    "kafka_df = (spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"retail-events\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write kafka_df to console\n",
    "query = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/data-lake/\") \\\n",
    "    .start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual data is encoded within a JSON object in the value column that we need to extract\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "eventSchema = (StructType()\n",
    "  .add('InvoiceNo', StringType())\n",
    "  .add('StockCode', StringType())\n",
    "  .add('Description', StringType())\n",
    "  .add('Quantity', IntegerType())\n",
    "  .add('InvoiceDate', StringType())\n",
    "  .add('UnitPrice', DoubleType())\n",
    "  .add('CustomerID', IntegerType())\n",
    "  .add('Country', StringType())\n",
    ")\n",
    "# we extract the data using the from_json() function by passing in the data schema object\n",
    "# This results in a retail_df DataFrame that has all of the columns of the event that we require\n",
    "# Additionally, we append an EventTime column from the Kafka topic, which shows when the event actually arrived in Kafka.\n",
    "# Since this DataFrame was created using the readStream() function it's a Streaming DataFrame and Structured Streaming APIs are available\n",
    "from pyspark.sql.functions import col, from_json, to_date\n",
    "retail_df = (kafka_df\\\n",
    "   .select(from_json(col(\"value\").cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\"))\\\n",
    "   .select(\"message.*\", \"EventTime\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write retail_df to console\n",
    "query = retail_df.writeStream.format(\"console\").option(\"checkpointLocation\", \"/tmp/data-lake/\").start()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/23 18:18:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff37c04c730>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Once we have extracted the raw event data from the Kafka stream, we can persist it to the data lake\n",
    "#\n",
    "# make use of the writeStream() function that is available to Streaming DataFrames to save data to the data lake in a streaming fashion\n",
    "# Once saved, these Parquet files are no different from any other Parquet files, whether created by batch processing or streams processing\n",
    "# Additionally, we use outputMode as append to indicate that we will treat this as an unbounded dataset and will keep appending new Parquet files.\n",
    "# The checkpointLocation option stores the Structured Streaming write-ahead log and other checkpointing information\n",
    "# This makes it an incremental data load job as the stream only picks up new and unprocessed events based on the offset information stored at the checkpoint location.\n",
    "#\n",
    "base_path = \"/tmp/data-lake/retail_events.parquet\"\n",
    "(retail_df\n",
    "    .withColumn(\"EventDate\", to_date(retail_df.EventTime))\n",
    "    .writeStream\n",
    "    .format('parquet')\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True)\n",
    "    .option('checkpointLocation', base_path + '/_checkpoint')\n",
    "    .start(base_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|           EventTime| EventDate|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|01/12/10 08:26|     2.55|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|01/12/10 08:26|     2.75|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|01/12/10 08:26|     7.65|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|01/12/10 08:26|     4.25|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|01/12/10 08:28|     1.85|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|01/12/10 08:28|     1.85|     17850|United Kingdom|2022-05-23 15:32:...|2022-05-23|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|01/12/10 08:26|     2.55|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|01/12/10 08:26|     2.75|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|01/12/10 08:26|     7.65|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|01/12/10 08:26|     4.25|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|01/12/10 08:28|     1.85|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|01/12/10 08:28|     1.85|     17850|United Kingdom|2022-05-23 15:34:...|2022-05-23|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|01/12/10 08:26|     2.55|     17850|United Kingdom|2022-05-23 15:44:...|2022-05-23|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|2022-05-23 15:44:...|2022-05-23|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"/tmp/data-lake/retail_events.parquet\").show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2678a815613a486355418d7a646a043051186984e97816ceb1e10bd784649dbc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
